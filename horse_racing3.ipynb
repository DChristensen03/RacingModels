{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import re\n",
    "import xmltodict\n",
    "from datetime import datetime\n",
    "\n",
    "def xml_to_dict(element):\n",
    "    \"\"\"Recursively converts an XML element and its children to a dictionary.\"\"\"\n",
    "    if len(element) == 0:\n",
    "        return element.text\n",
    "    return {child.tag: xml_to_dict(child) for child in element}\n",
    "\n",
    "def pretty(d, indent=0):\n",
    "   for key, value in d.items():\n",
    "      print('\\t' * indent + str(key))\n",
    "      if isinstance(value, dict):\n",
    "         pretty(value, indent+1)\n",
    "      else:\n",
    "         print('\\t' * (indent+1) + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and Parse XML Data using pandas.read_xml\n",
    "def load_performance_data(file_path):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    print(\"Loading PP data for: \", os.path.basename(file_path))\n",
    "\n",
    "    # Extract date from filename\n",
    "    date_match = re.search(r'\\d{8}', os.path.basename(file_path))\n",
    "    race_date = \"\"\n",
    "    if date_match:\n",
    "        race_date = date_match.group(0)\n",
    "\n",
    "    # Extract track from filename\n",
    "    track_name = os.path.basename(file_path).split('_')[0][-3:]\n",
    "    \n",
    "    # Extract each Race element within EntryRaceCard and convert to a dictionary\n",
    "    races = []\n",
    "    for race in root.findall('.//Race'):\n",
    "        race_dict = xmltodict.parse(ET.tostring(race))['Race']\n",
    "        if race_dict['BreedType']['Value'] != 'TB':\n",
    "            continue\n",
    "        race_dict = extract_general_race_info(race_dict, race_date, track_name)\n",
    "\n",
    "        for entry in race.findall('.//Starters'):\n",
    "            entry_dict = extract_entry_info(entry)\n",
    "            workout_dict = extract_workout_info(entry)\n",
    "            races.append({**race_dict, **entry_dict, **workout_dict})\n",
    "        \n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    df = pd.DataFrame(races)\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_general_race_info(race_dict, race_date, track_name):\n",
    "    # Extract date from filename\n",
    "    race_id = f\"{race_date}_{race_dict['RaceNumber']}_{track_name}\"\n",
    "    race_date = race_date\n",
    "    return {\n",
    "        \"race_id\": race_id,\n",
    "        \"course_type\": str(race_dict['Course']['Surface']['Value']),\n",
    "        \"distance\": int(race_dict['Distance']['DistanceId']),\n",
    "        \"race_type\": str(race_dict['RaceType']['RaceType']),\n",
    "        \"restriction_type\": str(race_dict['RestrictionType']['Value']),\n",
    "        \"condition\": str(race_dict['ConditionsOfRace']).strip(),\n",
    "        \"purse\": float(race_dict['PurseUSA']),\n",
    "        \"number_of_run\": int(race_dict['NumberOfRunners'])\n",
    "    }\n",
    "\n",
    "def extract_entry_info(entry_root):\n",
    "    entry_dict = xmltodict.parse(ET.tostring(entry_root))['Starters']\n",
    "\n",
    "    # Convert odds from fraction to decimal\n",
    "    odds_fraction = entry_dict['Odds']\n",
    "    if isinstance(odds_fraction, str) and '/' in odds_fraction:\n",
    "        numerator, denominator = map(float, odds_fraction.split('/'))\n",
    "        odds_decimal = numerator / denominator\n",
    "    else:\n",
    "        odds_decimal = float(odds_fraction) if odds_fraction else None\n",
    "    \n",
    "    final_dict = {\n",
    "        \"horse_id\": f\"{entry_dict['Horse']['HorseName']}_{entry_dict['ProgramNumber']}\",\n",
    "        \"gender\": str(entry_dict['Horse']['Sex']['Value']),\n",
    "        \"post_position\": int(entry_dict['PostPosition']),\n",
    "        \"weight\": int(entry_dict['WeightCarried']),\n",
    "        \"equipment\": str(entry_dict['Equipment']['Value']),\n",
    "        \"medication\": str(entry_dict['Medication']['Value']),\n",
    "        \"trainer\": int(entry_dict['Trainer']['ExternalPartyId']),\n",
    "        \"jockey\": int(entry_dict['Jockey']['ExternalPartyId']),\n",
    "        \"odds\": float(odds_decimal),\n",
    "    }\n",
    "\n",
    "\n",
    "    for i, pp in enumerate(entry_root.findall('.//PastPerformance')):\n",
    "        pp_dict = xmltodict.parse(ET.tostring(pp))['PastPerformance']\n",
    "        final_dict.update({\n",
    "            f\"pp_track_{i}\": str(pp_dict['Track']['TrackID']),\n",
    "            f\"pp_date_{i}\": datetime.strptime(pp_dict['RaceDate'][:10], '%Y-%m-%d'),\n",
    "            f\"pp_course_type_{i}\": str(pp_dict['Course']['Surface']['Value']),\n",
    "            f\"pp_distance_{i}\": int(pp_dict['Distance']['DistanceId']),\n",
    "            f\"pp_race_type_{i}\": str(pp_dict['RaceType']['RaceType']),\n",
    "            f\"pp_restriction_type_{i}\": str(pp_dict['RaceRestrictions']['RestrictionType']),\n",
    "            f\"pp_condition_{i}\": str(pp_dict['ConditionsOfRace']).strip(),\n",
    "            f\"pp_purse_{i}\": float(pp_dict['PurseUSA']),\n",
    "            f\"pp_number_of_run_{i}\": int(pp_dict['NumberOfStarters']),\n",
    "            f\"pp_finish_position_{i}\": int(pp_dict['Start']['OfficialFinish']),\n",
    "            f\"pp_class_rating_{i}\": int(pp_dict['Start']['ClassRating']),\n",
    "            f\"pp_speed_rating_{i}\": int(pp_dict['Start']['SpeedFigure']),\n",
    "        })\n",
    "\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "def extract_workout_info(entry_root):\n",
    "    final_dict = {}\n",
    "    for i, workout in enumerate(entry_root.findall('.//Workout')):\n",
    "        workout_dict = xmltodict.parse(ET.tostring(workout))['Workout']\n",
    "        final_dict.update({\n",
    "            f\"workout_date_{i}\": datetime.strptime(workout_dict['Date'][:10], '%Y-%m-%d'),\n",
    "            f\"workout_distance_{i}\": int(workout_dict['Distance']['DistanceId']),\n",
    "            f\"workout_course_type_{i}\": str(workout_dict['CourseType']['Surface']['Value']),\n",
    "            f\"workout_time_{i}\": int(workout_dict['Timing']),\n",
    "            f\"workout_rank_{i}\": int(workout_dict['Ranking']) / int(workout_dict['NumberInRankingGroup']),\n",
    "        })\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "performance_path = \"C:\\\\Users\\\\dylan\\\\OneDrive - Wayne State College\\\\Documents\\\\Equibase Data 2023\\\\2023 PPs\\\\Extracted PPs\"\n",
    "file_suffixes = ['PRM_USA.xml', 'CBY_USA.xml', 'FON_USA.xml']  # Add your suffixes here\n",
    "\n",
    "# Load all past performance files\n",
    "performance_data = pd.concat([load_performance_data(os.path.join(root, file)) \n",
    "                              for root, _, files in os.walk(performance_path) \n",
    "                              for file in files if file.endswith(tuple(file_suffixes))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results_data(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    print(\"Loading res data for: \", os.path.basename(file_path))\n",
    "\n",
    "    entries = []\n",
    "    for race in root.findall('.//RACE'):\n",
    "        race_date = re.search(r'\\d{8}', os.path.basename(file_path)).group(0)\n",
    "        race_number = race.get(\"NUMBER\")\n",
    "        track_name = os.path.basename(file_path).split('_')[0][:3].upper()\n",
    "\n",
    "        for entry in race.findall('.//ENTRY'):\n",
    "            horse_name = entry.find(\".//NAME\").text\n",
    "            horse_number = entry.find(\".//PROGRAM_NUM\").text\n",
    "            entry_data = {\n",
    "                \"horse_id\": f\"{horse_name}_{horse_number}\",\n",
    "                \"race_id\": f\"{race_date}_{race_number}_{track_name}\",\n",
    "                \"Position\": int(entry.find(\".//POINT_OF_CALL[@WHICH='FINAL']\").find('.//POSITION').text),\n",
    "            }\n",
    "            entries.append(entry_data)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "results_path = \"C:\\\\Users\\\\dylan\\\\OneDrive - Wayne State College\\\\Documents\\\\Equibase Data 2023\\\\2023 Result Charts\"\n",
    "file_prefixes = ['prm', 'cby', 'fon']  # Add your suffixes here\n",
    "\n",
    "\n",
    "# Load all results files\n",
    "all_races = []\n",
    "for file in os.listdir(results_path):\n",
    "    if file.endswith('.xml') and file.startswith(tuple(file_prefixes)):\n",
    "        file_path = os.path.join(results_path, file)\n",
    "        all_races.extend(load_results_data(file_path))\n",
    "\n",
    "results_data = pd.DataFrame(all_races)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Combine Data\n",
    "\n",
    "# Merge the DataFrames on RaceNumber and race_date\n",
    "merged_data = pd.merge(performance_data, results_data, on=[\"race_id\", \"horse_id\"], how='inner')\n",
    "\n",
    "# Clean up old DataFrames\n",
    "# del performance_data\n",
    "# del results_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any issues in the merged data\n",
    "print(merged_data.head())\n",
    "print(merged_data.info())\n",
    "\n",
    "# Select features and target variable\n",
    "features = merged_data.drop(columns=['Position'])\n",
    "target = merged_data['Position']\n",
    "\n",
    "# Verify the target variable\n",
    "print(target.describe())\n",
    "\n",
    "# Identify datetime features\n",
    "datetime_features = features.select_dtypes(include=['datetime64']).columns\n",
    "\n",
    "# Convert datetime features to numeric format\n",
    "for feature in datetime_features:\n",
    "    features[feature + '_year'] = features[feature].dt.year\n",
    "    features[feature + '_month'] = features[feature].dt.month\n",
    "    features[feature + '_day'] = features[feature].dt.day\n",
    "    features = features.drop(columns=[feature])\n",
    "\n",
    "# Identify categorical features\n",
    "categorical_features = features.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply one-hot encoding\n",
    "features = pd.get_dummies(features, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Handle missing values by filling them with the mean of the column\n",
    "features = features.fillna(features.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer instance to fill missing values\n",
    "imputer = SimpleImputer(strategy='mean')  # You can change the strategy as needed\n",
    "\n",
    "# Group past performances\n",
    "for i in range(10):\n",
    "    # get all columns beginning with pp and ending with i not containing interaction\n",
    "    columns_to_group = [col for col in features.columns if col.startswith('pp') and col.endswith(str(i)) and not col.startswith('pp_interaction')]\n",
    "\n",
    "    # Impute missing values\n",
    "    imputed_data = imputer.fit_transform(merged_data[columns_to_group])\n",
    "\n",
    "    # Create interaction terms\n",
    "    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "    interaction_terms = poly.fit_transform(imputed_data)\n",
    "\n",
    "    # Optionally, apply PCA to reduce dimensionality\n",
    "    pca = PCA(n_components=2)  # Adjust the number of components as needed\n",
    "    reduced_features = pca.fit_transform(interaction_terms)\n",
    "\n",
    "    for j in range(reduced_features.shape[1]):\n",
    "        features[f'pp_interaction_{i}_{j}'] = reduced_features[:, j]\n",
    "\n",
    "    merged_data = merged_data.drop(columns=columns_to_group)\n",
    "\n",
    "# Group workouts\n",
    "for i in range(4):\n",
    "    # get all columns beginning with workout and ending with i\n",
    "    columns_to_group = [col for col in features.columns if col.startswith('workout') and col.endswith(str(i)) and not col.startswith('workout_interaction')]\n",
    "\n",
    "    # Impute missing values\n",
    "    imputed_data = imputer.fit_transform(merged_data[columns_to_group])\n",
    "\n",
    "    # Create interaction terms\n",
    "    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "    interaction_terms = poly.fit_transform(imputed_data)\n",
    "\n",
    "    # Optionally, apply PCA to reduce dimensionality\n",
    "    pca = PCA(n_components=2)  # Adjust the number of components as needed\n",
    "    reduced_features = pca.fit_transform(interaction_terms)\n",
    "\n",
    "    for j in range(reduced_features.shape[1]):\n",
    "        features[f'workout_interaction_{i}_{j}'] = reduced_features[:, j]\n",
    "\n",
    "    merged_data = merged_data.drop(columns=columns_to_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', LinearSVC(max_iter=10000))\n",
    "])\n",
    "\n",
    "# Define the search space\n",
    "search_space = {\n",
    "    'svc__C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "    'svc__tol': Real(1e-6, 1e-1, prior='log-uniform')\n",
    "}\n",
    "\n",
    "# Initialize BayesSearchCV\n",
    "opt = BayesSearchCV(\n",
    "    estimator=pipeline,\n",
    "    search_spaces=search_space,\n",
    "    n_iter=32,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Perform the optimization\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found\n",
    "print(\"Best parameters found: \", opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "y_pred = opt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 8: Clip predictions to be within the range [1, number_of_run]\n",
    "number_of_run = X_test['number_of_run'].values  # Assuming 'number_of_run' is a column in X_test\n",
    "clipped_predictions = np.clip(y_pred, 1, number_of_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, clipped_predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "r2 = r2_score(y_test, clipped_predictions)\n",
    "print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Assuming 'odds' column is present in the test set\n",
    "odds = X_test['odds']\n",
    "\n",
    "# Step 8: Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, clipped_predictions, alpha=0.5, s=odds*10)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('True Values vs Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine y_test and y_pred into a single DataFrame\n",
    "results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'Odds': odds})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
