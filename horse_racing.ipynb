{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import re\n",
    "import xmltodict\n",
    "\n",
    "def xml_to_dict(element):\n",
    "    \"\"\"Recursively converts an XML element and its children to a dictionary.\"\"\"\n",
    "    if len(element) == 0:\n",
    "        return element.text\n",
    "    return {child.tag: xml_to_dict(child) for child in element}\n",
    "\n",
    "def pretty(d, indent=0):\n",
    "   for key, value in d.items():\n",
    "      print('\\t' * indent + str(key))\n",
    "      if isinstance(value, dict):\n",
    "         pretty(value, indent+1)\n",
    "      else:\n",
    "         print('\\t' * (indent+1) + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and Parse XML Data using pandas.read_xml\n",
    "def load_performance_data(file_path):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    print(\"Loading PP data for: \", os.path.basename(file_path))\n",
    "\n",
    "    # Extract date from filename\n",
    "    date_match = re.search(r'\\d{8}', os.path.basename(file_path))\n",
    "    race_date = \"\"\n",
    "    if date_match:\n",
    "        race_date = date_match.group(0)\n",
    "\n",
    "    # Extract track from filename\n",
    "    track_name = os.path.basename(file_path).split('_')[0][-3:]\n",
    "    \n",
    "    # Extract each Race element within EntryRaceCard and convert to a dictionary\n",
    "    races = []\n",
    "    for race in root.findall('.//Race'):\n",
    "        race_dict = xmltodict.parse(ET.tostring(race))['Race']\n",
    "        race_id = f\"{race_date}_{race_dict['RaceNumber']}_{track_name}\"\n",
    "        race_date = race_date\n",
    "        for entry in race.findall('.//Starters'):\n",
    "            entry_dict = xmltodict.parse(ET.tostring(entry))['Starters']\n",
    "            entry_dict['race_id'] = race_id\n",
    "            entry_dict['race_date'] = race_date\n",
    "            entry_dict['horse_id'] = f\"{entry_dict['Horse']['HorseName']}_{entry_dict['ProgramNumber']}\"\n",
    "            races.append(entry_dict)\n",
    "        \n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    df = pd.DataFrame(races)\n",
    "\n",
    "    return df\n",
    "\n",
    "performance_path = \"C:\\\\Users\\\\dylan\\\\OneDrive - Wayne State College\\\\Documents\\\\Equibase Data 2023\\\\2023 PPs\\\\Extracted PPs\"\n",
    "file_suffixes = ['PRM_USA.xml', 'CBY_USA.xml', 'FON_USA.xml']  # Add your suffixes here\n",
    "\n",
    "# Load all past performance files\n",
    "performance_data = pd.concat([load_performance_data(os.path.join(root, file)) \n",
    "                              for root, _, files in os.walk(performance_path) \n",
    "                              for file in files if file.endswith(tuple(file_suffixes))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results_data(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    print(\"Loading res data for: \", os.path.basename(file_path))\n",
    "\n",
    "    entries = []\n",
    "    for race in root.findall('.//RACE'):\n",
    "        race_date = re.search(r'\\d{8}', os.path.basename(file_path)).group(0)\n",
    "        race_number = race.get(\"NUMBER\")\n",
    "        track_name = os.path.basename(file_path).split('_')[0][:3].upper()\n",
    "\n",
    "        for entry in race.findall('.//ENTRY'):\n",
    "            horse_name = entry.find(\".//NAME\").text\n",
    "            horse_number = entry.find(\".//PROGRAM_NUM\").text\n",
    "            entry_data = {\n",
    "                \"horse_id\": f\"{horse_name}_{horse_number}\",\n",
    "                \"race_id\": f\"{race_date}_{race_number}_{track_name}\",\n",
    "                \"ProgramNumber\": horse_number,\n",
    "                \"HorseName\": horse_name,\n",
    "                \"Position\": entry.find(\".//POINT_OF_CALL[@WHICH='FINAL']\").find('.//POSITION').text,\n",
    "            }\n",
    "            entries.append(entry_data)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "results_path = \"C:\\\\Users\\\\dylan\\\\OneDrive - Wayne State College\\\\Documents\\\\Equibase Data 2023\\\\2023 Result Charts\"\n",
    "file_prefixes = ['prm', 'cby', 'fon']  # Add your suffixes here\n",
    "\n",
    "\n",
    "# Load all results files\n",
    "all_races = []\n",
    "for file in os.listdir(results_path):\n",
    "    if file.endswith('.xml') and file.startswith(tuple(file_prefixes)):\n",
    "        file_path = os.path.join(results_path, file)\n",
    "        all_races.extend(load_results_data(file_path))\n",
    "\n",
    "results_data = pd.DataFrame(all_races)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Combine Data\n",
    "\n",
    "# Merge the DataFrames on RaceNumber and race_date\n",
    "merged_data = pd.merge(performance_data, results_data, on=[\"race_id\", \"horse_id\"], how='inner')\n",
    "\n",
    "# Clean up old DataFrames\n",
    "del performance_data\n",
    "del results_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns if they exist\n",
    "cols_to_drop = ['WagerText', 'ParsedWagerText', 'ProgramSelections', 'RaceName', 'TrackRecord', 'SimulcastFlag']\n",
    "existing_cols_to_drop = [col for col in cols_to_drop if col in merged_data.columns]\n",
    "merged_data.drop(columns=existing_cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Data Preprocessing\n",
    "merged_data.ffill(inplace=True)  # Forward fill missing values\n",
    "\n",
    "# Check for remaining missing values\n",
    "print(\"Missing values after ffill:\")\n",
    "print(merged_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_columns(df):\n",
    "    def ensure_unique_columns(df):\n",
    "        # Ensure all column names are unique\n",
    "        cols = pd.Series(df.columns)\n",
    "        for dup in cols[cols.duplicated()].unique():\n",
    "            cols[cols[cols == dup].index.values.tolist()] = [dup + '_' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n",
    "        df.columns = cols\n",
    "        return df\n",
    "\n",
    "    # Find all columns that are type list\n",
    "    list_columns = df.columns[df.applymap(lambda x: isinstance(x, list)).any()]\n",
    "    \n",
    "    while len(list_columns) > 0:\n",
    "        for col in list_columns:\n",
    "            # Unpack list columns into new columns with a unique prefix\n",
    "            df = df.join(df[col].apply(pd.Series).add_prefix(col + '_'))\n",
    "            df = df.drop(columns=[col])\n",
    "        \n",
    "        # Ensure unique column names\n",
    "        df = ensure_unique_columns(df)\n",
    "        \n",
    "        # Find all columns that are type dict\n",
    "        dict_columns = df.columns[df.applymap(lambda x: isinstance(x, dict)).any()]\n",
    "        \n",
    "        while len(dict_columns) > 0:\n",
    "            for col in dict_columns:\n",
    "                flattened_df = pd.json_normalize(df[col])\n",
    "                \n",
    "                # Rename the columns to include the original column name as a prefix\n",
    "                flattened_df.columns = [f\"{col}_{subcol}\" for subcol in flattened_df.columns]\n",
    "                \n",
    "                # Drop the original dict column and concatenate the flattened columns\n",
    "                df = pd.concat([df.drop(columns=[col]), flattened_df], axis=1)\n",
    "            \n",
    "            # Ensure unique column names\n",
    "            df = ensure_unique_columns(df)\n",
    "            \n",
    "            # Update dict_columns after flattening\n",
    "            dict_columns = df.columns[df.applymap(lambda x: isinstance(x, dict)).any()]\n",
    "        \n",
    "        # Update list_columns after unpacking\n",
    "        list_columns = df.columns[df.applymap(lambda x: isinstance(x, list)).any()]\n",
    "    \n",
    "    return df\n",
    "\n",
    "merged_data = unpack_columns(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove all columns with less than 1% distinct values\n",
    "# threshold = 0.01\n",
    "# for col in merged_data.columns:\n",
    "#     count = merged_data[col].count()\n",
    "#     if count == 0:\n",
    "#         continue  # Skip columns with no values\n",
    "#     if merged_data[col].nunique() / count < threshold:\n",
    "#         merged_data.drop(columns=[col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all columns that are 100% empty\n",
    "merged_data.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all columns containing .FirstName .MiddleName .LastName and drop them\n",
    "name_columns = [col for col in merged_data.columns if '.FirstName' in col or '.MiddleName' in col or '.LastName' in col]\n",
    "merged_data.drop(columns=name_columns, inplace=True)\n",
    "del name_columns\n",
    "\n",
    "# Find all columns containing Sire.HorseName or Dam.HorseName and drop them\n",
    "pedigree_columns = [col for col in merged_data.columns if 'Sire.HorseName' in col or 'Dam.HorseName' in col]\n",
    "merged_data.drop(columns=pedigree_columns, inplace=True)\n",
    "\n",
    "# Find all columns containing Sire.FoalingDate or Dam.FoalingDate and drop them\n",
    "pedigree_columns = [col for col in merged_data.columns if 'Sire.FoalingDate' in col or 'Dam.FoalingDate' in col]\n",
    "merged_data.drop(columns=pedigree_columns, inplace=True)\n",
    "del pedigree_columns\n",
    "\n",
    "# Find all columns containing Trainer.ExternalPartyId and drop them\n",
    "trainer_columns = [col for col in merged_data.columns if 'Trainer.ExternalPartyId' in col]\n",
    "merged_data.drop(columns=trainer_columns, inplace=True)\n",
    "del trainer_columns\n",
    "\n",
    "# Find all columns containing Distance.PublishedValue and drop them\n",
    "distance_columns = [col for col in merged_data.columns if 'Distance.PublishedValue' in col]\n",
    "merged_data.drop(columns=distance_columns, inplace=True)\n",
    "del distance_columns\n",
    "\n",
    "# Find all columns containing CompanyLine and drop them\n",
    "company_columns = [col for col in merged_data.columns if 'CompanyLine' in col]\n",
    "merged_data.drop(columns=company_columns, inplace=True)\n",
    "del company_columns\n",
    "\n",
    "# Find all columns containing Scratch and drop them\n",
    "scratch_columns = [col for col in merged_data.columns if 'Scratch' in col]\n",
    "merged_data.drop(columns=scratch_columns, inplace=True)\n",
    "del scratch_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop list of columns if they exist\n",
    "cols_to_drop = ['RacingOwnerSilks', 'SaddleClothColor']\n",
    "existing_cols_to_drop = [col for col in cols_to_drop if col in merged_data.columns]\n",
    "merged_data.drop(columns=existing_cols_to_drop, inplace=True)\n",
    "del cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns containing the word 'Odds' from fraction to decimal\n",
    "for col in merged_data.columns:\n",
    "    if 'Odds' in col:\n",
    "        merged_data[col] = merged_data[col].apply(lambda x: eval(x.replace('/', '/')) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all columns that are strings but could be converted to a number and convert them\n",
    "for col in merged_data.columns:\n",
    "    if merged_data[col].apply(lambda x: isinstance(x, str) and x.replace('.', '', 1).isdigit()).all():\n",
    "        merged_data[col] = pd.to_numeric(merged_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all columns that are objects but could be converted to a number and convert them\n",
    "for col in merged_data.columns:\n",
    "    if merged_data[col].apply(lambda x: isinstance(x, object) and str(x).replace('.', '', 1).isdigit()).all():\n",
    "        merged_data[col] = pd.to_numeric(merged_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all columns containing the word 'Date' and convert them to datetime\n",
    "for col in merged_data.columns:\n",
    "    if 'Date' in col:\n",
    "        merged_data[col] = pd.to_datetime(merged_data[col], yearfirst=True, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all columns that are objects or strings but could be converted to a decimal and convert them\n",
    "for col in merged_data.columns:\n",
    "    if merged_data[col].apply(lambda x: x is None or (isinstance(x, object) and re.match(r'\\d+\\.\\d+', str(x)) is not None)).all():\n",
    "        merged_data[col] = pd.to_numeric(merged_data[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming merged_data is your DataFrame and 'Position' is the target column\n",
    "target = merged_data['Position']\n",
    "features = merged_data.drop(columns=['Position'])\n",
    "\n",
    "# Identify categorical and datetime columns\n",
    "categorical_cols = features.select_dtypes(include=['object']).columns\n",
    "datetime_cols = features.select_dtypes(include=['datetime64']).columns\n",
    "numeric_cols = features.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Convert datetime columns to numeric features (e.g., year, month, day)\n",
    "for col in datetime_cols:\n",
    "    features[col + '_year'] = features[col].dt.year\n",
    "    features[col + '_month'] = features[col].dt.month\n",
    "    features[col + '_day'] = features[col].dt.day\n",
    "\n",
    "# Drop the original datetime columns\n",
    "features = features.drop(columns=datetime_cols)\n",
    "\n",
    "# Fill None values in categorical columns with a placeholder (e.g., 'missing')\n",
    "features[categorical_cols] = features[categorical_cols].fillna('missing')\n",
    "\n",
    "# Create a column transformer with imputers and ordinal encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler(with_mean=False))\n",
    "        ]), numeric_cols),\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "        ]), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep the remaining columns as they are\n",
    ")\n",
    "\n",
    "# Transform and normalize the features\n",
    "normalized_features = preprocessor.fit_transform(features)\n",
    "\n",
    "# Check for any remaining NaN values and handle them\n",
    "if np.isnan(normalized_features).any():\n",
    "    # Option 1: Drop rows with NaN values\n",
    "    # normalized_features = normalized_features[~np.isnan(normalized_features).any(axis=1)]\n",
    "    \n",
    "    # Option 2: Fill remaining NaN values with a specific value (e.g., 0)\n",
    "    normalized_features = np.nan_to_num(normalized_features, nan=0.0)\n",
    "\n",
    "# Model Selection\n",
    "regressor = RandomForestRegressor(n_estimators=10, random_state=0, oob_score=True)\n",
    "\n",
    "# Training\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_features, target, test_size=0.2, random_state=42)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R²): {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot of actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
