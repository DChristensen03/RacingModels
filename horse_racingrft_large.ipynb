{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import re\n",
    "import xmltodict\n",
    "from datetime import datetime\n",
    "\n",
    "def xml_to_dict(element):\n",
    "    \"\"\"Recursively converts an XML element and its children to a dictionary.\"\"\"\n",
    "    if len(element) == 0:\n",
    "        return element.text\n",
    "    return {child.tag: xml_to_dict(child) for child in element}\n",
    "\n",
    "def pretty(d, indent=0):\n",
    "   for key, value in d.items():\n",
    "      print('\\t' * indent + str(key))\n",
    "      if isinstance(value, dict):\n",
    "         pretty(value, indent+1)\n",
    "      else:\n",
    "         print('\\t' * (indent+1) + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_quality_dict = {\n",
    "    \"TRL\": 1,\n",
    "    \"MCL\": 1,\n",
    "    \"WMC\": 1,\n",
    "    \"MOC\": 1.5,\n",
    "    \"MSA\": 1.5,\n",
    "    \"MSW\": 2.5,\n",
    "    \"WCL\": 2,\n",
    "    \"CLM\": 2,\n",
    "    \"MST\": 2.5,\n",
    "    \"CLH\": 2.5,\n",
    "    \"CST\": 2.5,\n",
    "    \"SOC\": 2.5,\n",
    "    \"OCL\":\t2.75,\n",
    "    \"SHP\":\t3,\n",
    "    \"STR\":\t2.75,\n",
    "    \"AOC\": 3.25,\n",
    "    \"OCS\": 3.5,\n",
    "    \"OCH\":\t3.25,\n",
    "    \"ALW\":\t4,\n",
    "    \"HCP\":\t4,\n",
    "    \"SIM\":\t2,\n",
    "    \"SST\":\t3.5,\n",
    "    \"STK\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PP data for:  SIMD20230102MVR_USA.xml\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 144\u001b[0m\n\u001b[0;32m    141\u001b[0m file_suffixes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMNR_USA.xml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCT_USA.xml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMVR_USA.xml\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Add your suffixes here\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Load all past performance files\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m performance_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([load_performance_data(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file)) \n\u001b[0;32m    145\u001b[0m                               \u001b[38;5;28;01mfor\u001b[39;00m root, _, files \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(performance_path) \n\u001b[0;32m    146\u001b[0m                               \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;28mtuple\u001b[39m(file_suffixes))])\n",
      "Cell \u001b[1;32mIn[4], line 144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    141\u001b[0m file_suffixes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMNR_USA.xml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCT_USA.xml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMVR_USA.xml\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Add your suffixes here\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Load all past performance files\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m performance_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([load_performance_data(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file)) \n\u001b[0;32m    145\u001b[0m                               \u001b[38;5;28;01mfor\u001b[39;00m root, _, files \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(performance_path) \n\u001b[0;32m    146\u001b[0m                               \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;28mtuple\u001b[39m(file_suffixes))])\n",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m, in \u001b[0;36mload_performance_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     24\u001b[0m race_dict, race_date \u001b[38;5;241m=\u001b[39m extract_general_race_info(race_dict, race_date, track_name)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m race\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.//Starters\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 27\u001b[0m     entry_dict \u001b[38;5;241m=\u001b[39m extract_entry_info(entry, race_date)\n\u001b[0;32m     28\u001b[0m     workout_dict \u001b[38;5;241m=\u001b[39m extract_workout_info(entry, race_date)\n\u001b[0;32m     29\u001b[0m     races\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrace_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mentry_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mworkout_dict})\n",
      "Cell \u001b[1;32mIn[4], line 52\u001b[0m, in \u001b[0;36mextract_entry_info\u001b[1;34m(entry_root, race_date)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_entry_info\u001b[39m(entry_root, race_date):\n\u001b[1;32m---> 52\u001b[0m     entry_dict \u001b[38;5;241m=\u001b[39m xmltodict\u001b[38;5;241m.\u001b[39mparse(ET\u001b[38;5;241m.\u001b[39mtostring(entry_root))[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# Convert odds from fraction to decimal\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     odds_fraction \u001b[38;5;241m=\u001b[39m entry_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOdds\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\dylan\\anaconda3\\Lib\\xml\\etree\\ElementTree.py:1098\u001b[0m, in \u001b[0;36mtostring\u001b[1;34m(element, encoding, method, xml_declaration, default_namespace, short_empty_elements)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate string representation of XML element.\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m \n\u001b[0;32m   1086\u001b[0m \u001b[38;5;124;03mAll subelements are included.  If encoding is \"unicode\", a string\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \n\u001b[0;32m   1096\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m stream \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mStringIO() \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124municode\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[1;32m-> 1098\u001b[0m ElementTree(element)\u001b[38;5;241m.\u001b[39mwrite(stream, encoding,\n\u001b[0;32m   1099\u001b[0m                            xml_declaration\u001b[38;5;241m=\u001b[39mxml_declaration,\n\u001b[0;32m   1100\u001b[0m                            default_namespace\u001b[38;5;241m=\u001b[39mdefault_namespace,\n\u001b[0;32m   1101\u001b[0m                            method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m   1102\u001b[0m                            short_empty_elements\u001b[38;5;241m=\u001b[39mshort_empty_elements)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32mc:\\Users\\dylan\\anaconda3\\Lib\\xml\\etree\\ElementTree.py:743\u001b[0m, in \u001b[0;36mElementTree.write\u001b[1;34m(self, file_or_filename, encoding, xml_declaration, default_namespace, method, short_empty_elements)\u001b[0m\n\u001b[0;32m    741\u001b[0m qnames, namespaces \u001b[38;5;241m=\u001b[39m _namespaces(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root, default_namespace)\n\u001b[0;32m    742\u001b[0m serialize \u001b[38;5;241m=\u001b[39m _serialize[method]\n\u001b[1;32m--> 743\u001b[0m serialize(write, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root, qnames, namespaces,\n\u001b[0;32m    744\u001b[0m           short_empty_elements\u001b[38;5;241m=\u001b[39mshort_empty_elements)\n",
      "File \u001b[1;32mc:\\Users\\dylan\\anaconda3\\Lib\\xml\\etree\\ElementTree.py:906\u001b[0m, in \u001b[0;36m_serialize_xml\u001b[1;34m(write, elem, qnames, namespaces, short_empty_elements, **kwargs)\u001b[0m\n\u001b[0;32m    904\u001b[0m         write(_escape_cdata(text))\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m elem:\n\u001b[1;32m--> 906\u001b[0m         _serialize_xml(write, e, qnames, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    907\u001b[0m                        short_empty_elements\u001b[38;5;241m=\u001b[39mshort_empty_elements)\n\u001b[0;32m    908\u001b[0m     write(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m tag \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dylan\\anaconda3\\Lib\\xml\\etree\\ElementTree.py:906\u001b[0m, in \u001b[0;36m_serialize_xml\u001b[1;34m(write, elem, qnames, namespaces, short_empty_elements, **kwargs)\u001b[0m\n\u001b[0;32m    904\u001b[0m         write(_escape_cdata(text))\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m elem:\n\u001b[1;32m--> 906\u001b[0m         _serialize_xml(write, e, qnames, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    907\u001b[0m                        short_empty_elements\u001b[38;5;241m=\u001b[39mshort_empty_elements)\n\u001b[0;32m    908\u001b[0m     write(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m tag \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dylan\\anaconda3\\Lib\\xml\\etree\\ElementTree.py:906\u001b[0m, in \u001b[0;36m_serialize_xml\u001b[1;34m(write, elem, qnames, namespaces, short_empty_elements, **kwargs)\u001b[0m\n\u001b[0;32m    904\u001b[0m         write(_escape_cdata(text))\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m elem:\n\u001b[1;32m--> 906\u001b[0m         _serialize_xml(write, e, qnames, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    907\u001b[0m                        short_empty_elements\u001b[38;5;241m=\u001b[39mshort_empty_elements)\n\u001b[0;32m    908\u001b[0m     write(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m tag \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dylan\\anaconda3\\Lib\\xml\\etree\\ElementTree.py:882\u001b[0m, in \u001b[0;36m_serialize_xml\u001b[1;34m(write, elem, qnames, namespaces, short_empty_elements, **kwargs)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     write(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m tag)\n\u001b[1;32m--> 882\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m items \u001b[38;5;129;01mor\u001b[39;00m namespaces:\n\u001b[0;32m    884\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m namespaces:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 2: Load and Parse XML Data using pandas.read_xml\n",
    "def load_performance_data(file_path):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    print(\"Loading PP data for: \", os.path.basename(file_path))\n",
    "\n",
    "    # Extract date from filename\n",
    "    date_match = re.search(r'\\d{8}', os.path.basename(file_path))\n",
    "    race_date = \"\"\n",
    "    if date_match:\n",
    "        race_date = date_match.group(0)\n",
    "\n",
    "    # Extract track from filename, remove numbers if short track name\n",
    "    track_name = os.path.basename(file_path).split('_')[0][-3:]\n",
    "    track_name = ''.join([i for i in track_name if not i.isdigit()])\n",
    "    \n",
    "    # Extract each Race element within EntryRaceCard and convert to a dictionary\n",
    "    races = []\n",
    "    for race in root.findall('.//Race'):\n",
    "        race_dict = xmltodict.parse(ET.tostring(race))['Race']\n",
    "        if race_dict['BreedType']['Value'] != 'TB':\n",
    "            continue\n",
    "        race_dict, race_date = extract_general_race_info(race_dict, race_date, track_name)\n",
    "\n",
    "        for entry in race.findall('.//Starters'):\n",
    "            entry_dict = extract_entry_info(entry, race_date)\n",
    "            workout_dict = extract_workout_info(entry, race_date)\n",
    "            races.append({**race_dict, **entry_dict, **workout_dict})\n",
    "        \n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    df = pd.DataFrame(races)\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_general_race_info(race_dict, race_date, track_name):\n",
    "    # Extract date from filename\n",
    "    race_id = f\"{race_date}_{race_dict['RaceNumber']}_{track_name}\"\n",
    "    race_date = race_date\n",
    "    return {\n",
    "        \"race_id\": race_id,\n",
    "        \"course_type\": str(race_dict['Course']['Surface']['Value']),\n",
    "        \"distance\": int(race_dict['Distance']['DistanceId']),\n",
    "        \"race_type\": str(race_dict['RaceType']['RaceType']),\n",
    "        \"restriction_type\": str(race_dict['RestrictionType']['Value']),\n",
    "        \"condition\": str(race_dict['ConditionsOfRace']).strip(),\n",
    "        \"purse\": float(race_dict['PurseUSA']),\n",
    "        \"number_of_run\": int(race_dict['NumberOfRunners'])\n",
    "    }, race_date\n",
    "\n",
    "def extract_entry_info(entry_root, race_date):\n",
    "    entry_dict = xmltodict.parse(ET.tostring(entry_root))['Starters']\n",
    "\n",
    "    # Convert odds from fraction to decimal\n",
    "    odds_fraction = entry_dict['Odds']\n",
    "    if isinstance(odds_fraction, str) and '/' in odds_fraction:\n",
    "        numerator, denominator = map(float, odds_fraction.split('/'))\n",
    "        odds_decimal = numerator / denominator\n",
    "    else:\n",
    "        odds_decimal = float(odds_fraction) if odds_fraction else None\n",
    "    \n",
    "    final_dict = {\n",
    "        \"horse_id\": f\"{entry_dict['Horse']['HorseName']}_{entry_dict['ProgramNumber']}\",\n",
    "        \"gender\": str(entry_dict['Horse']['Sex']['Value']),\n",
    "        \"post_position\": int(entry_dict['PostPosition']),\n",
    "        \"weight\": int(entry_dict['WeightCarried']),\n",
    "        \"equipment\": str(entry_dict['Equipment']['Value']),\n",
    "        \"medication\": str(entry_dict['Medication']['Value']),\n",
    "        \"trainer\": int(entry_dict['Trainer']['ExternalPartyId']),\n",
    "        \"jockey\": int(entry_dict['Jockey']['ExternalPartyId']),\n",
    "        \"odds\": (odds_decimal),\n",
    "    }\n",
    "\n",
    "    summaries = entry_root.findall('.//RaceSummary')\n",
    "    if summaries is list:\n",
    "        summary_dict = xmltodict.parse(ET.tostring(summaries[0]))['RaceSummary']\n",
    "        final_dict.update({\n",
    "            \"win_percentage_year\": float(summary_dict['NumberOfWins']) / float(summary_dict['NumberOfStarts']),\n",
    "            \"otb_percentage_year\": (float(summary_dict['NumberOfWins']) + float(summary_dict['NumberOfSeconds']) + float(summary_dict['NumberOfThirds'])) / float(summary_dict['NumberOfStarts']),\n",
    "        })\n",
    "\n",
    "\n",
    "    for i, pp in enumerate(entry_root.findall('.//PastPerformance')):\n",
    "        if i > 5:\n",
    "            break\n",
    "        pp_dict = xmltodict.parse(ET.tostring(pp))['PastPerformance']\n",
    "\n",
    "        race_type = str(pp_dict['RaceType']['RaceType'])\n",
    "        race_quality = race_quality_dict[race_type] if str(race_type) in race_quality_dict.keys() else 1\n",
    "\n",
    "        if str(pp_dict['RaceRestrictions']['RestrictionType']) == 'S':\n",
    "            race_quality -= 1\n",
    "        if race_type == 'STK' and pp_dict['Grade'] is not None:\n",
    "            race_quality += 4 - int(pp_dict['Grade'])\n",
    "\n",
    "        if i == 0:\n",
    "            final_dict.update({\n",
    "                \"pp_layoff\": (datetime.strptime(race_date, '%Y%m%d') - datetime.strptime(pp_dict['RaceDate'][:10], '%Y-%m-%d')).days\n",
    "            })\n",
    "\n",
    "        bad_luck = False\n",
    "        long_comment = str(pp_dict['Start']['LongComment']).lower()\n",
    "        if long_comment is not None:\n",
    "            if any(['bump' in long_comment, 'stumbled' in long_comment, 'checked' in long_comment, 'steadied' in long_comment, 'stopped' in long_comment, 'squeezed' in long_comment, 'steady' in long_comment or 'steadied' in long_comment, 'head turned' in long_comment, 'unprepared start' in long_comment, 'wd' in long_comment or 'wide' in long_comment, 'bled' in long_comment]):\n",
    "                bad_luck = True\n",
    "\n",
    "        final_dict.update({\n",
    "            f\"pp_track_{i}\": str(pp_dict['Track']['TrackID']),\n",
    "            f\"pp_time_since_race_{i}\": (datetime.strptime(race_date, '%Y%m%d') - datetime.strptime(pp_dict['RaceDate'][:10], '%Y-%m-%d')).days,\n",
    "            f\"pp_course_type_{i}\": str(pp_dict['Course']['Surface']['Value']),\n",
    "            f\"pp_distance_{i}\": int(pp_dict['Distance']['DistanceId']),\n",
    "            f\"pp_quality_{i}\": race_quality,\n",
    "            f\"pp_purse_{i}\": float(pp_dict['PurseUSA']),\n",
    "            f\"pp_normalized_position_{i}\": np.divide(float(pp_dict['Start']['OfficialFinish']), float(pp_dict['NumberOfStarters'])),\n",
    "            f\"pp_class_rating_{i}\": int(pp_dict['Start']['ClassRating']),\n",
    "            f\"pp_speed_rating_{i}\": int(pp_dict['Start']['SpeedFigure']),\n",
    "            f\"pp_pace_figure_1_{i}\": int(pp_dict['Start']['PaceFigure1']),\n",
    "            f\"pp_pace_figure_2_{i}\": int(pp_dict['Start']['PaceFigure2']),\n",
    "            f\"pp_pace_figure_3_{i}\": int(pp_dict['Start']['PaceFigure3']),\n",
    "            f\"pp_bad_luck_{i}\": bad_luck,\n",
    "        })\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "def extract_workout_info(entry_root, race_date):\n",
    "    final_dict = {}\n",
    "    for i, workout in enumerate(entry_root.findall('.//Workout')):        \n",
    "        workout_dict = xmltodict.parse(ET.tostring(workout))['Workout']\n",
    "        final_dict.update({\n",
    "            f\"workout_last_month_{i}\": True if (datetime.strptime(race_date, '%Y%m%d')  - datetime.strptime(workout_dict['Date'][:10], '%Y-%m-%d')).days < 30 else False,\n",
    "            f\"workout_distance_{i}\": int(workout_dict['Distance']['DistanceId']),\n",
    "            f\"workout_course_type_{i}\": str(workout_dict['CourseType']['Surface']['Value']),\n",
    "            f\"workout_time_{i}\": int(workout_dict['Timing']),\n",
    "            f\"workout_rank_{i}\": int(workout_dict['Ranking']) / int(workout_dict['NumberInRankingGroup']),\n",
    "        })\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "performance_path = \"C:\\\\Users\\\\dylan\\\\OneDrive - Wayne State College\\\\Documents\\\\Equibase Data 2023\\\\2023 PPs\\\\Extracted PPs\"\n",
    "file_suffixes = ['MNR_USA.xml', 'CT_USA.xml']  # Add your suffixes here\n",
    "\n",
    "# Load all past performance files\n",
    "performance_data = pd.concat([load_performance_data(os.path.join(root, file)) \n",
    "                              for root, _, files in os.walk(performance_path) \n",
    "                              for file in files if file.endswith(tuple(file_suffixes))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading res data for:  ct20230104tch.xml\n",
      "Loading res data for:  ct20230105tch.xml\n",
      "Loading res data for:  ct20230106tch.xml\n",
      "Loading res data for:  ct20230107tch.xml\n",
      "Loading res data for:  ct20230111tch.xml\n",
      "Loading res data for:  ct20230112tch.xml\n",
      "Loading res data for:  ct20230113tch.xml\n",
      "Loading res data for:  ct20230114tch.xml\n",
      "Loading res data for:  ct20230118tch.xml\n",
      "Loading res data for:  ct20230119tch.xml\n",
      "Loading res data for:  ct20230120tch.xml\n",
      "Loading res data for:  ct20230121tch.xml\n",
      "Loading res data for:  ct20230125tch.xml\n",
      "Loading res data for:  ct20230126tch.xml\n",
      "Loading res data for:  ct20230127tch.xml\n",
      "Loading res data for:  ct20230128tch.xml\n",
      "Loading res data for:  ct20230201tch.xml\n",
      "Loading res data for:  ct20230202tch.xml\n",
      "Loading res data for:  ct20230204tch.xml\n",
      "Loading res data for:  ct20230208tch.xml\n",
      "Loading res data for:  ct20230209tch.xml\n",
      "Loading res data for:  ct20230210tch.xml\n",
      "Loading res data for:  ct20230211tch.xml\n",
      "Loading res data for:  ct20230215tch.xml\n",
      "Loading res data for:  ct20230216tch.xml\n",
      "Loading res data for:  ct20230217tch.xml\n",
      "Loading res data for:  ct20230218tch.xml\n",
      "Loading res data for:  ct20230222tch.xml\n",
      "Loading res data for:  ct20230223tch.xml\n",
      "Loading res data for:  ct20230224tch.xml\n",
      "Loading res data for:  ct20230225tch.xml\n",
      "Loading res data for:  ct20230301tch.xml\n",
      "Loading res data for:  ct20230302tch.xml\n",
      "Loading res data for:  ct20230303tch.xml\n",
      "Loading res data for:  ct20230304tch.xml\n",
      "Loading res data for:  ct20230309tch.xml\n",
      "Loading res data for:  ct20230310tch.xml\n",
      "Loading res data for:  ct20230311tch.xml\n",
      "Loading res data for:  ct20230316tch.xml\n",
      "Loading res data for:  ct20230317tch.xml\n",
      "Loading res data for:  ct20230318tch.xml\n",
      "Loading res data for:  ct20230323tch.xml\n",
      "Loading res data for:  ct20230324tch.xml\n",
      "Loading res data for:  ct20230325tch.xml\n",
      "Loading res data for:  ct20230330tch.xml\n",
      "Loading res data for:  ct20230331tch.xml\n",
      "Loading res data for:  ct20230401tch.xml\n",
      "Loading res data for:  ct20230406tch.xml\n",
      "Loading res data for:  ct20230407tch.xml\n",
      "Loading res data for:  ct20230408tch.xml\n",
      "Loading res data for:  ct20230413tch.xml\n",
      "Loading res data for:  ct20230414tch.xml\n",
      "Loading res data for:  ct20230415tch.xml\n",
      "Loading res data for:  ct20230420tch.xml\n",
      "Loading res data for:  ct20230421tch.xml\n",
      "Loading res data for:  ct20230422tch.xml\n",
      "Loading res data for:  ct20230427tch.xml\n",
      "Loading res data for:  ct20230428tch.xml\n",
      "Loading res data for:  ct20230429tch.xml\n",
      "Loading res data for:  ct20230504tch.xml\n",
      "Loading res data for:  ct20230505tch.xml\n",
      "Loading res data for:  ct20230506tch.xml\n",
      "Loading res data for:  ct20230511tch.xml\n",
      "Loading res data for:  ct20230512tch.xml\n",
      "Loading res data for:  ct20230513tch.xml\n",
      "Loading res data for:  ct20230518tch.xml\n",
      "Loading res data for:  ct20230519tch.xml\n",
      "Loading res data for:  ct20230520tch.xml\n",
      "Loading res data for:  ct20230525tch.xml\n",
      "Loading res data for:  ct20230526tch.xml\n",
      "Loading res data for:  ct20230527tch.xml\n",
      "Loading res data for:  ct20230601tch.xml\n",
      "Loading res data for:  ct20230602tch.xml\n",
      "Loading res data for:  ct20230603tch.xml\n",
      "Loading res data for:  ct20230609tch.xml\n",
      "Loading res data for:  ct20230610tch.xml\n",
      "Loading res data for:  ct20230615tch.xml\n",
      "Loading res data for:  ct20230616tch.xml\n",
      "Loading res data for:  ct20230617tch.xml\n",
      "Loading res data for:  ct20230622tch.xml\n",
      "Loading res data for:  ct20230623tch.xml\n",
      "Loading res data for:  ct20230624tch.xml\n",
      "Loading res data for:  ct20230630tch.xml\n",
      "Loading res data for:  ct20230701tch.xml\n",
      "Loading res data for:  ct20230706tch.xml\n",
      "Loading res data for:  ct20230707tch.xml\n",
      "Loading res data for:  ct20230708tch.xml\n",
      "Loading res data for:  ct20230713tch.xml\n",
      "Loading res data for:  ct20230714tch.xml\n",
      "Loading res data for:  ct20230715tch.xml\n",
      "Loading res data for:  ct20230720tch.xml\n",
      "Loading res data for:  ct20230721tch.xml\n",
      "Loading res data for:  ct20230722tch.xml\n",
      "Loading res data for:  ct20230727tch.xml\n",
      "Loading res data for:  ct20230728tch.xml\n",
      "Loading res data for:  ct20230729tch.xml\n",
      "Loading res data for:  ct20230803tch.xml\n",
      "Loading res data for:  ct20230804tch.xml\n",
      "Loading res data for:  ct20230805tch.xml\n",
      "Loading res data for:  ct20230810tch.xml\n",
      "Loading res data for:  ct20230811tch.xml\n",
      "Loading res data for:  ct20230812tch.xml\n",
      "Loading res data for:  ct20230817tch.xml\n",
      "Loading res data for:  ct20230818tch.xml\n",
      "Loading res data for:  ct20230819tch.xml\n",
      "Loading res data for:  ct20230824tch.xml\n",
      "Loading res data for:  ct20230825tch.xml\n",
      "Loading res data for:  ct20230826tch.xml\n",
      "Loading res data for:  ct20230831tch.xml\n",
      "Loading res data for:  ct20230901tch.xml\n",
      "Loading res data for:  ct20230902tch.xml\n",
      "Loading res data for:  ct20230907tch.xml\n",
      "Loading res data for:  ct20230908tch.xml\n",
      "Loading res data for:  ct20230909tch.xml\n",
      "Loading res data for:  ct20230914tch.xml\n",
      "Loading res data for:  ct20230915tch.xml\n",
      "Loading res data for:  ct20230916tch.xml\n",
      "Loading res data for:  ct20230921tch.xml\n",
      "Loading res data for:  ct20230922tch.xml\n",
      "Loading res data for:  ct20230923tch.xml\n",
      "Loading res data for:  ct20230928tch.xml\n",
      "Loading res data for:  ct20230929tch.xml\n",
      "Loading res data for:  ct20230930tch.xml\n",
      "Loading res data for:  ct20231005tch.xml\n",
      "Loading res data for:  ct20231006tch.xml\n",
      "Loading res data for:  ct20231007tch.xml\n",
      "Loading res data for:  ct20231012tch.xml\n",
      "Loading res data for:  ct20231013tch.xml\n",
      "Loading res data for:  ct20231014tch.xml\n",
      "Loading res data for:  ct20231018tch.xml\n",
      "Loading res data for:  ct20231019tch.xml\n",
      "Loading res data for:  ct20231020tch.xml\n",
      "Loading res data for:  ct20231021tch.xml\n",
      "Loading res data for:  ct20231025tch.xml\n",
      "Loading res data for:  ct20231026tch.xml\n",
      "Loading res data for:  ct20231027tch.xml\n",
      "Loading res data for:  ct20231028tch.xml\n",
      "Loading res data for:  ct20231101tch.xml\n",
      "Loading res data for:  ct20231102tch.xml\n",
      "Loading res data for:  ct20231103tch.xml\n",
      "Loading res data for:  ct20231104tch.xml\n",
      "Loading res data for:  ct20231108tch.xml\n",
      "Loading res data for:  ct20231109tch.xml\n",
      "Loading res data for:  ct20231110tch.xml\n",
      "Loading res data for:  ct20231111tch.xml\n",
      "Loading res data for:  ct20231115tch.xml\n",
      "Loading res data for:  ct20231116tch.xml\n",
      "Loading res data for:  ct20231117tch.xml\n",
      "Loading res data for:  ct20231118tch.xml\n",
      "Loading res data for:  ct20231122tch.xml\n",
      "Loading res data for:  ct20231124tch.xml\n",
      "Loading res data for:  ct20231125tch.xml\n",
      "Loading res data for:  ct20231129tch.xml\n",
      "Loading res data for:  ct20231130tch.xml\n",
      "Loading res data for:  ct20231201tch.xml\n",
      "Loading res data for:  ct20231202tch.xml\n",
      "Loading res data for:  ct20231206tch.xml\n",
      "Loading res data for:  ct20231207tch.xml\n",
      "Loading res data for:  ct20231208tch.xml\n",
      "Loading res data for:  ct20231209tch.xml\n",
      "Loading res data for:  ct20231213tch.xml\n",
      "Loading res data for:  ct20231214tch.xml\n",
      "Loading res data for:  ct20231215tch.xml\n",
      "Loading res data for:  ct20231216tch.xml\n",
      "Loading res data for:  ctd20230909tch.xml\n",
      "Loading res data for:  ctd20230910tch.xml\n",
      "Loading res data for:  ctd20230916tch.xml\n",
      "Loading res data for:  ctd20230917tch.xml\n",
      "Loading res data for:  ctd20230923tch.xml\n",
      "Loading res data for:  ctd20230924tch.xml\n",
      "Loading res data for:  ctd20230930tch.xml\n",
      "Loading res data for:  ctd20231001tch.xml\n",
      "Loading res data for:  ctd20231007tch.xml\n",
      "Loading res data for:  ctd20231008tch.xml\n",
      "Loading res data for:  ctd20231014tch.xml\n",
      "Loading res data for:  ctd20231015tch.xml\n",
      "Loading res data for:  ctd20231021tch.xml\n",
      "Loading res data for:  ctd20231022tch.xml\n",
      "Loading res data for:  ctm20230506tch.xml\n",
      "Loading res data for:  ctm20230512tch.xml\n",
      "Loading res data for:  ctm20230513tch.xml\n",
      "Loading res data for:  ctm20230526tch.xml\n",
      "Loading res data for:  ctm20230527tch.xml\n",
      "Loading res data for:  ctm20230602tch.xml\n",
      "Loading res data for:  ctm20230603tch.xml\n",
      "Loading res data for:  ctm20230609tch.xml\n",
      "Loading res data for:  ctm20230610tch.xml\n",
      "Loading res data for:  ctm20230616tch.xml\n",
      "Loading res data for:  ctm20230617tch.xml\n",
      "Loading res data for:  ctm20230623tch.xml\n",
      "Loading res data for:  ctm20230624tch.xml\n",
      "Loading res data for:  ctm20230630tch.xml\n",
      "Loading res data for:  ctm20230701tch.xml\n",
      "Loading res data for:  ctm20230707tch.xml\n",
      "Loading res data for:  ctm20230708tch.xml\n",
      "Loading res data for:  ctm20230709tch.xml\n",
      "Loading res data for:  ctm20230716tch.xml\n",
      "Loading res data for:  ctm20230721tch.xml\n",
      "Loading res data for:  ctm20230722tch.xml\n",
      "Loading res data for:  ctm20230723tch.xml\n",
      "Loading res data for:  ctm20230728tch.xml\n",
      "Loading res data for:  ctm20230729tch.xml\n",
      "Loading res data for:  ctm20230730tch.xml\n",
      "Loading res data for:  ctm20230804tch.xml\n",
      "Loading res data for:  ctm20230805tch.xml\n",
      "Loading res data for:  ctm20230806tch.xml\n",
      "Loading res data for:  ctm20230811tch.xml\n",
      "Loading res data for:  ctm20230812tch.xml\n",
      "Loading res data for:  ctm20230813tch.xml\n",
      "Loading res data for:  ctm20230818tch.xml\n",
      "Loading res data for:  ctm20230819tch.xml\n",
      "Loading res data for:  ctm20230825tch.xml\n",
      "Loading res data for:  ctm20230826tch.xml\n",
      "Loading res data for:  ctm20230901tch.xml\n",
      "Loading res data for:  mnr20230430tch.xml\n",
      "Loading res data for:  mnr20230501tch.xml\n",
      "Loading res data for:  mnr20230502tch.xml\n",
      "Loading res data for:  mnr20230507tch.xml\n",
      "Loading res data for:  mnr20230508tch.xml\n",
      "Loading res data for:  mnr20230509tch.xml\n",
      "Loading res data for:  mnr20230514tch.xml\n",
      "Loading res data for:  mnr20230515tch.xml\n",
      "Loading res data for:  mnr20230516tch.xml\n",
      "Loading res data for:  mnr20230521tch.xml\n",
      "Loading res data for:  mnr20230522tch.xml\n",
      "Loading res data for:  mnr20230523tch.xml\n",
      "Loading res data for:  mnr20230528tch.xml\n",
      "Loading res data for:  mnr20230529tch.xml\n",
      "Loading res data for:  mnr20230530tch.xml\n",
      "Loading res data for:  mnr20230604tch.xml\n",
      "Loading res data for:  mnr20230605tch.xml\n",
      "Loading res data for:  mnr20230606tch.xml\n",
      "Loading res data for:  mnr20230611tch.xml\n",
      "Loading res data for:  mnr20230613tch.xml\n",
      "Loading res data for:  mnr20230618tch.xml\n",
      "Loading res data for:  mnr20230619tch.xml\n",
      "Loading res data for:  mnr20230620tch.xml\n",
      "Loading res data for:  mnr20230621tch.xml\n",
      "Loading res data for:  mnr20230625tch.xml\n",
      "Loading res data for:  mnr20230626tch.xml\n",
      "Loading res data for:  mnr20230627tch.xml\n",
      "Loading res data for:  mnr20230702tch.xml\n",
      "Loading res data for:  mnr20230703tch.xml\n",
      "Loading res data for:  mnr20230705tch.xml\n",
      "Loading res data for:  mnr20230709tch.xml\n",
      "Loading res data for:  mnr20230710tch.xml\n",
      "Loading res data for:  mnr20230711tch.xml\n",
      "Loading res data for:  mnr20230712tch.xml\n",
      "Loading res data for:  mnr20230716tch.xml\n",
      "Loading res data for:  mnr20230717tch.xml\n",
      "Loading res data for:  mnr20230718tch.xml\n",
      "Loading res data for:  mnr20230719tch.xml\n",
      "Loading res data for:  mnr20230723tch.xml\n",
      "Loading res data for:  mnr20230724tch.xml\n",
      "Loading res data for:  mnr20230725tch.xml\n",
      "Loading res data for:  mnr20230726tch.xml\n",
      "Loading res data for:  mnr20230730tch.xml\n",
      "Loading res data for:  mnr20230731tch.xml\n",
      "Loading res data for:  mnr20230801tch.xml\n",
      "Loading res data for:  mnr20230802tch.xml\n",
      "Loading res data for:  mnr20230806tch.xml\n",
      "Loading res data for:  mnr20230807tch.xml\n",
      "Loading res data for:  mnr20230808tch.xml\n",
      "Loading res data for:  mnr20230809tch.xml\n",
      "Loading res data for:  mnr20230813tch.xml\n",
      "Loading res data for:  mnr20230814tch.xml\n",
      "Loading res data for:  mnr20230815tch.xml\n",
      "Loading res data for:  mnr20230816tch.xml\n",
      "Loading res data for:  mnr20230820tch.xml\n",
      "Loading res data for:  mnr20230821tch.xml\n",
      "Loading res data for:  mnr20230822tch.xml\n",
      "Loading res data for:  mnr20230823tch.xml\n",
      "Loading res data for:  mnr20230827tch.xml\n",
      "Loading res data for:  mnr20230828tch.xml\n",
      "Loading res data for:  mnr20230829tch.xml\n",
      "Loading res data for:  mnr20230830tch.xml\n",
      "Loading res data for:  mnr20230903tch.xml\n",
      "Loading res data for:  mnr20230904tch.xml\n",
      "Loading res data for:  mnr20230905tch.xml\n",
      "Loading res data for:  mnr20230906tch.xml\n",
      "Loading res data for:  mnr20230910tch.xml\n",
      "Loading res data for:  mnr20230911tch.xml\n",
      "Loading res data for:  mnr20230912tch.xml\n",
      "Loading res data for:  mnr20230913tch.xml\n",
      "Loading res data for:  mnr20230917tch.xml\n",
      "Loading res data for:  mnr20230918tch.xml\n",
      "Loading res data for:  mnr20230919tch.xml\n",
      "Loading res data for:  mnr20230920tch.xml\n",
      "Loading res data for:  mnr20230924tch.xml\n",
      "Loading res data for:  mnr20230925tch.xml\n",
      "Loading res data for:  mnr20230926tch.xml\n",
      "Loading res data for:  mnr20230927tch.xml\n",
      "Loading res data for:  mnr20231001tch.xml\n",
      "Loading res data for:  mnr20231002tch.xml\n",
      "Loading res data for:  mnr20231003tch.xml\n",
      "Loading res data for:  mnr20231004tch.xml\n",
      "Loading res data for:  mnr20231008tch.xml\n",
      "Loading res data for:  mnr20231009tch.xml\n",
      "Loading res data for:  mnr20231010tch.xml\n",
      "Loading res data for:  mnr20231011tch.xml\n",
      "Loading res data for:  mnr20231015tch.xml\n",
      "Loading res data for:  mnr20231016tch.xml\n",
      "Loading res data for:  mnr20231017tch.xml\n",
      "Loading res data for:  mnr20231018tch.xml\n",
      "Loading res data for:  mnr20231022tch.xml\n",
      "Loading res data for:  mnr20231023tch.xml\n",
      "Loading res data for:  mnr20231024tch.xml\n",
      "Loading res data for:  mnr20231025tch.xml\n",
      "Loading res data for:  mnr20231029tch.xml\n",
      "Loading res data for:  mnr20231030tch.xml\n",
      "Loading res data for:  mnr20231031tch.xml\n",
      "Loading res data for:  mnr20231101tch.xml\n",
      "Loading res data for:  mnr20231105tch.xml\n",
      "Loading res data for:  mnr20231106tch.xml\n",
      "Loading res data for:  mnr20231107tch.xml\n",
      "Loading res data for:  mnr20231108tch.xml\n",
      "Loading res data for:  mnr20231112tch.xml\n",
      "Loading res data for:  mnr20231113tch.xml\n",
      "Loading res data for:  mnr20231114tch.xml\n",
      "Loading res data for:  mnr20231115tch.xml\n",
      "Loading res data for:  mnr20231119tch.xml\n",
      "Loading res data for:  mnr20231120tch.xml\n",
      "Loading res data for:  mnr20231121tch.xml\n",
      "Loading res data for:  mnr20231122tch.xml\n",
      "Loading res data for:  mnr20231126tch.xml\n",
      "Loading res data for:  mnr20231127tch.xml\n",
      "Loading res data for:  mnr20231129tch.xml\n",
      "Loading res data for:  mnr20231203tch.xml\n",
      "Loading res data for:  mnr20231204tch.xml\n",
      "Loading res data for:  mnr20231205tch.xml\n",
      "Loading res data for:  mnr20231206tch.xml\n",
      "Loading res data for:  mnr20231210tch.xml\n",
      "Loading res data for:  mnr20231211tch.xml\n",
      "Loading res data for:  mnr20231212tch.xml\n",
      "Loading res data for:  mnr20231213tch.xml\n",
      "Loading res data for:  mvr20230102tch.xml\n",
      "Loading res data for:  mvr20230103tch.xml\n",
      "Loading res data for:  mvr20230104tch.xml\n",
      "Loading res data for:  mvr20230105tch.xml\n",
      "Loading res data for:  mvr20230109tch.xml\n",
      "Loading res data for:  mvr20230110tch.xml\n",
      "Loading res data for:  mvr20230111tch.xml\n",
      "Loading res data for:  mvr20230112tch.xml\n",
      "Loading res data for:  mvr20230116tch.xml\n",
      "Loading res data for:  mvr20230117tch.xml\n",
      "Loading res data for:  mvr20230118tch.xml\n",
      "Loading res data for:  mvr20230119tch.xml\n",
      "Loading res data for:  mvr20230123tch.xml\n",
      "Loading res data for:  mvr20230124tch.xml\n",
      "Loading res data for:  mvr20230126tch.xml\n",
      "Loading res data for:  mvr20230128tch.xml\n",
      "Loading res data for:  mvr20230130tch.xml\n",
      "Loading res data for:  mvr20230131tch.xml\n",
      "Loading res data for:  mvr20230201tch.xml\n",
      "Loading res data for:  mvr20230202tch.xml\n",
      "Loading res data for:  mvr20230206tch.xml\n",
      "Loading res data for:  mvr20230207tch.xml\n",
      "Loading res data for:  mvr20230208tch.xml\n",
      "Loading res data for:  mvr20230209tch.xml\n",
      "Loading res data for:  mvr20230213tch.xml\n",
      "Loading res data for:  mvr20230214tch.xml\n",
      "Loading res data for:  mvr20230215tch.xml\n",
      "Loading res data for:  mvr20230216tch.xml\n",
      "Loading res data for:  mvr20230220tch.xml\n",
      "Loading res data for:  mvr20230221tch.xml\n",
      "Loading res data for:  mvr20230222tch.xml\n",
      "Loading res data for:  mvr20230223tch.xml\n",
      "Loading res data for:  mvr20230227tch.xml\n",
      "Loading res data for:  mvr20230228tch.xml\n",
      "Loading res data for:  mvr20230301tch.xml\n",
      "Loading res data for:  mvr20230302tch.xml\n",
      "Loading res data for:  mvr20230306tch.xml\n",
      "Loading res data for:  mvr20230307tch.xml\n",
      "Loading res data for:  mvr20230308tch.xml\n",
      "Loading res data for:  mvr20230309tch.xml\n",
      "Loading res data for:  mvr20230313tch.xml\n",
      "Loading res data for:  mvr20230314tch.xml\n",
      "Loading res data for:  mvr20230315tch.xml\n",
      "Loading res data for:  mvr20230316tch.xml\n",
      "Loading res data for:  mvr20230320tch.xml\n",
      "Loading res data for:  mvr20230321tch.xml\n",
      "Loading res data for:  mvr20230322tch.xml\n",
      "Loading res data for:  mvr20230323tch.xml\n",
      "Loading res data for:  mvr20230327tch.xml\n",
      "Loading res data for:  mvr20230328tch.xml\n",
      "Loading res data for:  mvr20230329tch.xml\n",
      "Loading res data for:  mvr20230330tch.xml\n",
      "Loading res data for:  mvr20230403tch.xml\n",
      "Loading res data for:  mvr20230404tch.xml\n",
      "Loading res data for:  mvr20230405tch.xml\n",
      "Loading res data for:  mvr20230406tch.xml\n",
      "Loading res data for:  mvr20230410tch.xml\n",
      "Loading res data for:  mvr20230411tch.xml\n",
      "Loading res data for:  mvr20230412tch.xml\n",
      "Loading res data for:  mvr20230413tch.xml\n",
      "Loading res data for:  mvr20230415tch.xml\n",
      "Loading res data for:  mvr20231021tch.xml\n",
      "Loading res data for:  mvr20231023tch.xml\n",
      "Loading res data for:  mvr20231024tch.xml\n",
      "Loading res data for:  mvr20231025tch.xml\n",
      "Loading res data for:  mvr20231028tch.xml\n",
      "Loading res data for:  mvr20231030tch.xml\n",
      "Loading res data for:  mvr20231031tch.xml\n",
      "Loading res data for:  mvr20231101tch.xml\n",
      "Loading res data for:  mvr20231102tch.xml\n",
      "Loading res data for:  mvr20231106tch.xml\n",
      "Loading res data for:  mvr20231107tch.xml\n",
      "Loading res data for:  mvr20231108tch.xml\n",
      "Loading res data for:  mvr20231109tch.xml\n",
      "Loading res data for:  mvr20231113tch.xml\n",
      "Loading res data for:  mvr20231114tch.xml\n",
      "Loading res data for:  mvr20231115tch.xml\n",
      "Loading res data for:  mvr20231116tch.xml\n",
      "Loading res data for:  mvr20231120tch.xml\n",
      "Loading res data for:  mvr20231121tch.xml\n",
      "Loading res data for:  mvr20231122tch.xml\n",
      "Loading res data for:  mvr20231127tch.xml\n",
      "Loading res data for:  mvr20231128tch.xml\n",
      "Loading res data for:  mvr20231129tch.xml\n",
      "Loading res data for:  mvr20231130tch.xml\n",
      "Loading res data for:  mvr20231204tch.xml\n",
      "Loading res data for:  mvr20231205tch.xml\n",
      "Loading res data for:  mvr20231206tch.xml\n",
      "Loading res data for:  mvr20231207tch.xml\n",
      "Loading res data for:  mvr20231211tch.xml\n",
      "Loading res data for:  mvr20231212tch.xml\n",
      "Loading res data for:  mvr20231213tch.xml\n",
      "Loading res data for:  mvr20231214tch.xml\n",
      "Loading res data for:  mvr20231218tch.xml\n",
      "Loading res data for:  mvr20231219tch.xml\n",
      "Loading res data for:  mvr20231220tch.xml\n",
      "Loading res data for:  mvr20231221tch.xml\n",
      "Loading res data for:  mvr20231226tch.xml\n",
      "Loading res data for:  mvr20231227tch.xml\n",
      "Loading res data for:  mvr20231228tch.xml\n",
      "Loading res data for:  mvr20231230tch.xml\n"
     ]
    }
   ],
   "source": [
    "def load_results_data(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    print(\"Loading res data for: \", os.path.basename(file_path))\n",
    "\n",
    "    entries = []\n",
    "    for race in root.findall('.//RACE'):\n",
    "        race_date = re.search(r'\\d{8}', os.path.basename(file_path)).group(0)\n",
    "        race_number = race.get(\"NUMBER\")\n",
    "        track_name = os.path.basename(file_path).split('_')[0][:3].upper()\n",
    "        track_name = ''.join([i for i in track_name if not i.isdigit()])\n",
    "\n",
    "        for entry in race.findall('.//ENTRY'):\n",
    "            horse_name = entry.find(\".//NAME\").text\n",
    "            horse_number = entry.find(\".//PROGRAM_NUM\").text\n",
    "            entry_data = {\n",
    "                \"horse_id\": f\"{horse_name}_{horse_number}\",\n",
    "                \"race_id\": f\"{race_date}_{race_number}_{track_name}\",\n",
    "                \"Position\": int(entry.find(\".//POINT_OF_CALL[@WHICH='FINAL']\").find('.//POSITION').text),\n",
    "            }\n",
    "            entries.append(entry_data)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "results_path = \"C:\\\\Users\\\\dylan\\\\OneDrive - Wayne State College\\\\Documents\\\\Equibase Data 2023\\\\2023 Result Charts\"\n",
    "file_prefixes = ['mnr', 'ct']  # Add your suffixes here\n",
    "\n",
    "\n",
    "# Load all results files\n",
    "all_races = []\n",
    "for file in os.listdir(results_path):\n",
    "    if file.endswith('.xml') and file.startswith(tuple(file_prefixes)):\n",
    "        file_path = os.path.join(results_path, file)\n",
    "        all_races.extend(load_results_data(file_path))\n",
    "\n",
    "results_data = pd.DataFrame(all_races)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Combine Data\n",
    "\n",
    "# Merge the DataFrames on RaceNumber and race_date\n",
    "merged_data = pd.merge(performance_data, results_data, on=[\"race_id\", \"horse_id\"], how='inner')\n",
    "\n",
    "# Create normalized position\n",
    "merged_data['normalized_position'] = np.multiply(np.divide(merged_data['Position'], merged_data['number_of_run']), 100)\n",
    "\n",
    "# Clean up old DataFrames\n",
    "# del performance_data\n",
    "# del results_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Go through each row and create new columns\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m---> 58\u001b[0m     futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(process_row, i, row) \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m merged_data\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(futures):\n\u001b[0;32m     60\u001b[0m         i \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_data' is not defined"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "jockeys = {}\n",
    "trainers = {}\n",
    "jockey_trainer = {}\n",
    "\n",
    "def process_row(i, row):\n",
    "    # Initialize new columns\n",
    "    merged_data.at[i, \"first_off_layoff\"] = False\n",
    "    merged_data.at[i, \"second_off_layoff\"] = False\n",
    "    merged_data.at[i, \"third_off_layoff\"] = False\n",
    "    \n",
    "    # Set if horse is off layoff\n",
    "    if row['pp_time_since_race_0'] > 45:\n",
    "        merged_data.at[i, 'first_off_layoff'] = True\n",
    "    elif row['pp_time_since_race_1'] - row['pp_time_since_race_0'] > 45:\n",
    "        merged_data.at[i, 'second_off_layoff'] = True\n",
    "    elif row['pp_time_since_race_2'] - row['pp_time_since_race_1'] > 45:\n",
    "        merged_data.at[i, 'third_off_layoff'] = True\n",
    "\n",
    "    # Get jockey trainer win percentage\n",
    "    # Find each jockey row with the same value and check if the position is 1\n",
    "    jockey_id = merged_data.at[i, 'jockey']\n",
    "    if (jockey_id not in jockeys):\n",
    "        jockey_win_percentage = merged_data.groupby('jockey')['Position'].transform(lambda x: (x == 1).sum() / len(x))\n",
    "        jockeys[jockey_id] = jockey_win_percentage\n",
    "    else:\n",
    "        jockey_win_percentage = jockeys[jockey_id]\n",
    "    # Divide that by the total rows with that jockey id\n",
    "    merged_data['jockey_win_percentage'] = jockey_win_percentage\n",
    "\n",
    "\n",
    "    # Find each trainer row with the same value and check if the position is 1\n",
    "    trainer_id = merged_data.at[i, 'trainer']\n",
    "    if (trainer_id not in trainers):\n",
    "        trainer_win_percentage = merged_data.groupby('trainer')['Position'].transform(lambda x: (x == 1).sum() / len(x))\n",
    "        trainers[trainer_id] = trainer_win_percentage\n",
    "    else:\n",
    "        trainer_win_percentage = trainers[trainer_id]\n",
    "    # Divide that by the total rows with that trainer id\n",
    "    merged_data['trainer_win_percentage'] = trainer_win_percentage\n",
    "\n",
    "    # Find each trainer row and jockey row with the same value and check if the position is 1\n",
    "    jockey_trainer_id = f\"{merged_data.at[i, 'jockey']}{merged_data.at[i, 'trainer']}\"\n",
    "    if (jockey_trainer_id not in jockey_trainer):\n",
    "        trainer_jockey_win_percentage = merged_data.groupby(['trainer', 'jockey'])['Position'].transform(lambda x: (x == 1).sum() / len(x))\n",
    "        jockey_trainer[jockey_trainer_id] = trainer_jockey_win_percentage\n",
    "    else:\n",
    "        trainer_jockey_win_percentage = jockey_trainer[jockey_trainer_id]\n",
    "\n",
    "    # Divide that by the total rows with that jockey and trainer id\n",
    "    merged_data['trainer_jockey_win_percentage'] = trainer_jockey_win_percentage\n",
    "\n",
    "    return i\n",
    "\n",
    "# Go through each row and create new columns\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_row, i, row) for i, row in merged_data.iterrows()]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        i = future.result()\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "merged_data.reset_index()\n",
    "\n",
    "# Drop unneeded columns\n",
    "# merged_data = merged_data.drop(columns=['race_id', 'horse_id'])\n",
    "\n",
    "# Identify columns with missing values\n",
    "columns_with_missing = merged_data.columns[merged_data.isnull().any()]\n",
    "\n",
    "# Impute only columns with missing values\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputed_array = imputer.fit_transform(merged_data[columns_with_missing])\n",
    "\n",
    "# Convert the imputed array back to a DataFrame with original column names\n",
    "imputed_data = pd.DataFrame(imputed_array, columns=columns_with_missing)\n",
    "\n",
    "# Combine the imputed columns with the rest of the data\n",
    "merged_data_imputed = merged_data.copy()\n",
    "merged_data_imputed[columns_with_missing] = imputed_data\n",
    "\n",
    "# Use LabelEncoder on string columns\n",
    "label_encoder = LabelEncoder()\n",
    "for col in merged_data_imputed.columns:\n",
    "    if merged_data_imputed[col].dtype == 'object':\n",
    "        merged_data_imputed[col] = label_encoder.fit_transform(merged_data_imputed[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to csv\n",
    "merged_data_imputed.to_csv(\"C:\\\\Users\\\\dylan\\\\OneDrive - Wayne State College\\\\Documents\\\\Scrubbed 2023 Data\\\\WV.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = merged_data_imputed.drop(columns=['normalized_position', 'Position'])\n",
    "y = merged_data_imputed['normalized_position']\n",
    "groups = merged_data_imputed['race_id']\n",
    "\n",
    "# Split the data\n",
    "gss = GroupShuffleSplit(test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "# Store the 'odds' column for later use\n",
    "odds_train = X_train['odds']\n",
    "odds_test = X_test['odds']\n",
    "\n",
    "# Drop the 'odds' column from the training and testing datasets\n",
    "X_train = X_train.drop(columns=['odds'])\n",
    "X_test = X_test.drop(columns=['odds'])\n",
    "\n",
    "# Initialize the model with possible hyperparameters\n",
    "# param_grid = {\n",
    "#     'max_depth': [2,4,8,10,12,16,24],\n",
    "#     'max_features': [0.5,1,40,50,60,80,100],\n",
    "#     'min_samples_leaf': [2,4,10,11,12,15,18],\n",
    "#     'n_estimators': [10,20,50150,175,200,225,250]\n",
    "# }\n",
    "# grid_search = GridSearchCV(estimator=RandomForestRegressor(n_jobs=-1),\n",
    "#                            param_grid=param_grid,\n",
    "#                            cv=5,  # 5-fold cross-validation\n",
    "#                            n_jobs=-1,  # Use all available cores\n",
    "#                            verbose=3)\n",
    "model = RandomForestRegressor(n_jobs=-1, max_depth=20, max_features=.5, min_samples_leaf=10, n_estimators=300)\n",
    "\n",
    "# Train the model\n",
    "model_results = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best parameters and model\n",
    "# best_params = grid_search.best_params_\n",
    "# best_model = grid_search.best_estimator_\n",
    "# print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_normalized_position = model.predict(X_test)\n",
    "predicted_finish_position = ((predicted_normalized_position * X_test['number_of_run']) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming imputed_merged_data and predicted_finish_position are already defined\n",
    "\n",
    "# Extract actual finish positions\n",
    "actual_finish_position = (y_test * X_test['number_of_run']) / 100\n",
    "\n",
    "# Extract and normalize the Odds column\n",
    "odds = odds_test\n",
    "normalized_odds = (odds - odds.min()) / (odds.max() - odds.min()) * 100  # Scale to a range of 0 to 100\n",
    "\n",
    "# Plot actual vs predicted finish positions with bubble sizes relative to Odds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(actual_finish_position, predicted_finish_position, s=normalized_odds, color='red', alpha=0.1, label='Predicted vs Actual')\n",
    "plt.plot([actual_finish_position.min(), actual_finish_position.max()], \n",
    "         [actual_finish_position.min(), actual_finish_position.max()], \n",
    "         'r--', lw=2, label='Ideal Fit')\n",
    "plt.xlabel('Actual Finish Position')\n",
    "plt.ylabel('Predicted Finish Position')\n",
    "plt.title('Actual vs Predicted Finish Positions')\n",
    "plt.legend()\n",
    "\n",
    "# Set the tick labels to show every number\n",
    "plt.xticks(range(int(actual_finish_position.min()), int(actual_finish_position.max()) + 1))\n",
    "plt.yticks(range(int(predicted_finish_position.min()), int(predicted_finish_position.max()) + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame by concatenating the series\n",
    "results_df = pd.concat([X_test['race_id'], actual_finish_position, predicted_finish_position, odds], axis=1)\n",
    "\n",
    "# Rename the columns for clarity\n",
    "results_df.columns = ['race_id', 'actual_finish_position', 'predicted_finish_position', 'odds']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another dataframe from results_df that selects the lowest predicted_finish_position for each race_id\n",
    "best_predictions = results_df.groupby('race_id').agg({'predicted_finish_position': 'min'}).reset_index()\n",
    "# Merge the best_predictions dataframe with results_df to get all other columns\n",
    "merged_best_predictions = pd.merge(best_predictions, results_df, on=[\"race_id\", \"predicted_finish_position\"], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_predictions = results_df.groupby('race_id').apply(lambda x: x.nsmallest(3, 'predicted_finish_position')).reset_index(drop=True)\n",
    "top_3_predictions = pd.merge(top_3_predictions, results_df, on=['race_id', 'predicted_finish_position'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Assuming actual_finish_position and predicted_finish_position are already defined\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(actual_finish_position, predicted_finish_position)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared value\n",
    "r2 = r2_score(actual_finish_position, predicted_finish_position)\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Calculate mean absolute error\n",
    "mae = mean_absolute_error(actual_finish_position, predicted_finish_position)\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "# Mean Squared Error: 4.554393896236332\n",
    "# R-squared: 0.32095816070573235\n",
    "# Mean Absolute Error: 1.7526350751142776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_file = \"C:\\\\Users\\\\dylan\\\\OneDrive - Wayne State College\\\\Documents\\\\2023 Models\\\\West Virginia.csv\"\n",
    "\n",
    "with open(model_file, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
